import asyncio
import sys
import os
from typing import Annotated, TypedDict

from langchain_core.messages import AnyMessage, HumanMessage, AIMessage
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.runnables import RunnableLambda
from langgraph.graph import StateGraph, END

# Hugging Face ë° LangChain ì—°ë™ì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ import
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
from langchain_huggingface import HuggingFacePipeline

# MCP ë° ë¡œì»¬ ë„êµ¬ import
from mcp.client.session import ClientSession
from mcp.client.stdio import StdioServerParameters, stdio_client
from mcp_tools import MCPToolWrapper

from langchain_core.utils.function_calling import convert_to_openai_tool
import json


# --- 1. Agentì˜ ìƒíƒœ ì •ì˜ ---
# Agentê°€ ì‘ì—…ì„ ìˆ˜í–‰í•˜ëŠ” ë™ì•ˆ ì¶”ì í•´ì•¼ í•  ì •ë³´ (ê¸°ì¡´ê³¼ ë™ì¼)
class AgentState(TypedDict):
    messages: Annotated[list[AnyMessage], lambda x, y: x + y]


# --- Helper Function ---
def construct_aimessage_from_llm_output(output: str) -> AIMessage:
    """
    LLMì˜ ì›ì‹œ ë¬¸ìì—´ ì¶œë ¥ì„ íŒŒì‹±í•©ë‹ˆë‹¤.
    - ì¶œë ¥ì´ tool_callsë¥¼ í¬í•¨í•œ ìœ íš¨í•œ JSONì´ë©´, tool_callsë¥¼ í¬í•¨í•œ AIMessageë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    - ê·¸ë ‡ì§€ ì•Šìœ¼ë©´, ì›ì‹œ ë¬¸ìì—´ì„ contentë¡œ í•˜ëŠ” ì¼ë°˜ AIMessageë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    ì´ëŠ” ëª¨ë¸ì´ í•­ìƒ ì™„ë²½í•œ JSONì„ ì¶œë ¥í•˜ì§€ ì•ŠëŠ” ì‹¤ì œ ìƒí™©ì— ëŒ€ì‘í•˜ê¸° ìœ„í•¨ì…ë‹ˆë‹¤.
    """
    try:
        # ëª¨ë¸ì´ ```json ... ``` ì½”ë“œ ë¸”ë¡ìœ¼ë¡œ ê°ì‹¸ì„œ ì¶œë ¥í•˜ëŠ” ê²½ìš°ê°€ ë§ìŠµë‹ˆë‹¤.
        if "```json" in output:
            json_str = output.split("```json")[1].strip().rstrip("`")
        else:
            json_str = output

        parsed_json = json.loads(json_str)

        if isinstance(parsed_json, dict) and "tool_calls" in parsed_json and isinstance(parsed_json["tool_calls"], list):
            # ìœ íš¨í•œ ë„êµ¬ í˜¸ì¶œ JSONì„ ì°¾ì•˜ìŠµë‹ˆë‹¤.
            return AIMessage(content="", tool_calls=parsed_json["tool_calls"])
        else:
            # JSONì´ê¸´ í•˜ì§€ë§Œ ì˜ˆìƒëœ í˜•ì‹ì´ ì•„ë‹™ë‹ˆë‹¤.
            return AIMessage(content=str(parsed_json))
    except (json.JSONDecodeError, IndexError):
        # JSON íŒŒì‹±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ì¼ë°˜ í…ìŠ¤íŠ¸ ì‘ë‹µìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.
        return AIMessage(content=output)


# --- 2. í”„ë¡¬í”„íŠ¸ ì •ì˜ ---
# LLMì—ê²Œ ì—­í• ê³¼ ì§€ì¹¨, ê·¸ë¦¬ê³  ë„êµ¬ ì‚¬ìš©ë²•ì„ ë¶€ì—¬í•©ë‹ˆë‹¤.
system_prompt = """ë‹¹ì‹ ì€ ìš°ë¦¬ì€í–‰ì˜ ìœ ëŠ¥í•œ ëŒ€ì¶œ ì‹¬ì‚¬ ë³´ì¡° AI ì—ì´ì „íŠ¸ì…ë‹ˆë‹¤.
ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µí•˜ê¸° ìœ„í•´ ì£¼ì–´ì§„ ë„êµ¬ë¥¼ ì‚¬ìš©í•´ì•¼ í• ì§€ íŒë‹¨í•˜ê³ , í•„ìš”í•˜ë‹¤ë©´ ì ì ˆí•œ ë„êµ¬ë¥¼ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤.

ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ ëª©ë¡ì…ë‹ˆë‹¤:
{tools}

ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ë ¤ë©´, ë°˜ë“œì‹œ ë‹¤ìŒ JSON í˜•ì‹ì— ë§ì¶° `tool_calls` í‚¤ë¥¼ í¬í•¨í•˜ì—¬ ì‘ë‹µí•´ì•¼ í•©ë‹ˆë‹¤. ë‹¤ë¥¸ í…ìŠ¤íŠ¸ëŠ” í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.

```json
{{
    "tool_calls": [
        {{
            "name": "read_applicant_info",
            "args": {{
                "applicant_id": "A001"
            }},
            "id": "tool_call_123"
        }}
    ]
}}
```

ë„êµ¬ ì‹¤í–‰ ê²°ê³¼ê°€ ì£¼ì–´ì§€ë©´, ê·¸ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì—ê²Œ ìµœì¢… ë‹µë³€ì„ ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ ë¬¸ì¥ìœ¼ë¡œ ìƒì„±í•˜ì„¸ìš”.
ì ˆëŒ€ ë„êµ¬ ì‹¤í–‰ ê²°ê³¼ë¥¼ ê·¸ëŒ€ë¡œ ë³´ì—¬ì£¼ì–´ì„œëŠ” ì•ˆ ë©ë‹ˆë‹¤. í•­ìƒ ì‚¬ëŒì—ê²Œ ì„¤ëª…í•˜ë“¯ì´ ìš”ì•½í•˜ê³  ì •ë¦¬í•´ì„œ ë‹µë³€í•´ì•¼ í•©ë‹ˆë‹¤.

### ì‘ì—… ì ˆì°¨
- **ë‹¨ìˆœ ì •ë³´ ì¡°íšŒ**: ì‚¬ìš©ìê°€ ë‹¨ìˆœíˆ ì§€ì›ì ì •ë³´ ì¡°íšŒë¥¼ ìš”ì²­í•˜ë©´ `read_applicant_info` ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.
- **ëŒ€ì¶œ ì‹¬ì‚¬**: ì‚¬ìš©ìê°€ 'ì‹¬ì‚¬', 'í‰ê°€', 'ìŠ¹ì¸' ë“± ëŒ€ì¶œ ê²°ì •ê³¼ ê´€ë ¨ëœ ìš”ì²­ì„ í•˜ë©´, **ë°˜ë“œì‹œ ë‹¤ìŒ ë‘ ë‹¨ê³„ë¥¼ ìˆœì„œëŒ€ë¡œ ì§„í–‰í•´ì•¼ í•©ë‹ˆë‹¤.**
    1. **ë¨¼ì € `read_applicant_info`ë¥¼ ì‚¬ìš©**í•˜ì—¬ ì§€ì›ìì˜ ì „ì²´ ì •ë³´ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
    2. **ê·¸ ë‹¤ìŒ, `evaluate_loan_application`ë¥¼ í˜¸ì¶œ**í•˜ì—¬ ìµœì¢… ì‹¬ì‚¬ë¥¼ ì™„ë£Œí•˜ì„¸ìš”. ì´ ë‹¨ê³„ ì—†ì´ëŠ” ì ˆëŒ€ ì‹¬ì‚¬ ê²°ê³¼ë¥¼ ë‹µë³€í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.
"""

prompt_template = ChatPromptTemplate.from_messages(
    [
        ("system", system_prompt),
        MessagesPlaceholder(variable_name="messages"),
    ]
)


# --- 3. LLM ë° ë„êµ¬ ì„¤ì • ---
async def setup_resources():
    """SmolLM3 ëª¨ë¸ê³¼ MCP í´ë¼ì´ì–¸íŠ¸ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤."""
    # LLM ë¡œë“œ (SmolLM3-3B)
    model_id = "HuggingFaceTB/SmolLM3-3B"

    # ì°¸ê³ : GPU ì‚¬ìš©ì„ ìœ„í•´ device="cuda"ë¡œ ì„¤ì •í•˜ì„¸ìš”. CPUë§Œ ì‚¬ìš© ì‹œ "cpu"ë¡œ ë³€ê²½í•©ë‹ˆë‹¤.
    # torch_dtype="auto"ëŠ” ê°€ëŠ¥í•œ ê²½ìš° ìµœì ì˜ ë°ì´í„° íƒ€ì…ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
    model = AutoModelForCausalLM.from_pretrained(model_id, device_map="cuda", torch_dtype="auto")
    tokenizer = AutoTokenizer.from_pretrained(model_id)

    # Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ pipeline ìƒì„±
    pipe = pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer,
        max_new_tokens=2048,
        temperature=0.0,  # ì¼ê´€ì„± ìˆëŠ” ì¶œë ¥ì„ ìœ„í•´ 0ìœ¼ë¡œ ì„¤ì •
    )

    # LangChainê³¼ í˜¸í™˜ë˜ë„ë¡ HuggingFacePipelineìœ¼ë¡œ ë˜í•‘
    llm = HuggingFacePipeline(pipeline=pipe)

    # MCP í´ë¼ì´ì–¸íŠ¸ ì„¸ì…˜ ì‹œì‘
    server_params = StdioServerParameters(
        command=sys.executable,
        args=["server.py"],
        env=dict(os.environ, PYTHONUNBUFFERED="1")
    )

    return llm, server_params


# --- 4. LangGraph ì •ì˜ ---
def should_continue(state: AgentState) -> str:
    """Toolì„ í˜¸ì¶œí• ì§€, ì•„ë‹ˆë©´ ì‚¬ìš©ìì—ê²Œ ë‹µë³€í•˜ê³  ì¢…ë£Œí• ì§€ ê²°ì •"""
    # ë§ˆì§€ë§‰ ë©”ì‹œì§€ì— tool_callsê°€ ìˆëŠ”ì§€ í™•ì¸
    if state["messages"][-1].tool_calls:
        return "call_tool"
    return "end"


# --- 5. ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ ---
async def main():
    llm, server_params = await setup_resources()

    async with stdio_client(server_params) as (read, write):
        async with ClientSession(read, write) as client:
            await client.initialize()
            print("âœ… MCP ì„¸ì…˜ ì´ˆê¸°í™” ì„±ê³µ!")

            mcp_tool_wrapper = MCPToolWrapper(client)
            tools = mcp_tool_wrapper.get_tools()

            # SmolLMê³¼ ê°™ì€ ì¼ë°˜ í…ìŠ¤íŠ¸ ìƒì„± ëª¨ë¸ì€ `bind_tools`ë¥¼ ì§ì ‘ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
            # ëŒ€ì‹ , í”„ë¡¬í”„íŠ¸ì— ë„êµ¬ ì •ë³´ë¥¼ ëª…ì‹œì ìœ¼ë¡œ ì£¼ì…í•˜ê³ , ëª¨ë¸ì˜ ì¶œë ¥(JSON ë¬¸ìì—´)ì„ íŒŒì‹±í•˜ëŠ” ê³¼ì •ì„ ì§ì ‘ êµ¬ì„±í•´ì•¼ í•©ë‹ˆë‹¤.

            # 1. LangChain ë„êµ¬ë¥¼ ëª¨ë¸ì´ ì´í•´í•  ìˆ˜ ìˆëŠ” JSON ìŠ¤í‚¤ë§ˆë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
            tool_schemas = [convert_to_openai_tool(t) for t in tools]

            # 2. LLMì˜ ì¶œë ¥ì„ íŒŒì‹±í•˜ê³  ì ì ˆí•œ AIMessageë¥¼ ìƒì„±í•˜ëŠ” ì‹¤í–‰ ì²´ì¸ì„ êµ¬ì„±í•©ë‹ˆë‹¤.
            agent_runnable = (
                {
                    "messages": lambda x: x["messages"],
                    # `json.dumps`ë¥¼ ì‚¬ìš©í•˜ì—¬ ë„êµ¬ ìŠ¤í‚¤ë§ˆë¥¼ í”„ë¡¬í”„íŠ¸ì— ì£¼ì…í•  ë¬¸ìì—´ë¡œ ë§Œë“­ë‹ˆë‹¤.
                    "tools": lambda x: json.dumps(tool_schemas, indent=2, ensure_ascii=False),
                }
                | prompt_template
                | llm
                # LLMì˜ ì›ì‹œ ì¶œë ¥ì„ íŒŒì‹±í•˜ì—¬ AIMessage(tool_calls í¬í•¨ ê°€ëŠ¥)ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
                | RunnableLambda(construct_aimessage_from_llm_output)
            )

            # --- LangGraph ë…¸ë“œ ì •ì˜ ---
            def call_model(state: AgentState):
                """LLMì„ í˜¸ì¶œí•˜ì—¬ ë‹¤ìŒ ë‹¨ê³„ë¥¼ ê²°ì •"""
                # .invoke()ë¥¼ ì‚¬ìš©í•˜ì—¬ ì—°ê²°ëœ ì²´ì¸ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.
                response = agent_runnable.invoke(state)
                return {"messages": [response]}

            async def call_tool(state: AgentState):
                """LLMì´ í˜¸ì¶œí•˜ê¸°ë¡œ ê²°ì •í•œ ë„êµ¬ë¥¼ ì‹¤ì œë¡œ ì‹¤í–‰"""
                tool_results = []
                last_message = state["messages"][-1]
                for tool_call in last_message.tool_calls:
                    tool_name = tool_call["name"]
                    selected_tool = next((t for t in tools if t.name == tool_name), None)
                    if not selected_tool:
                        error_msg = f"Tool '{tool_name}' not found."
                        tool_results.append(AIMessage(content=error_msg, tool_call_id=tool_call["id"]))
                        continue

                    # ë„êµ¬ì˜ ë¹„ë™ê¸° ë©”ì„œë“œ 'ainvoke'ë¥¼ 'await'í•˜ì—¬ ì‹¤í–‰
                    observation = await selected_tool.ainvoke(tool_call["args"])

                    tool_results.append(
                        AIMessage(content=str(observation), tool_call_id=tool_call["id"])
                    )
                return {"messages": tool_results}

            # ê·¸ë˜í”„ ìƒì„± (ê¸°ì¡´ê³¼ ë™ì¼)
            graph = StateGraph(AgentState)
            graph.add_node("agent", call_model)
            graph.add_node("call_tool", call_tool)
            graph.set_entry_point("agent")
            graph.add_conditional_edges("agent", should_continue, {"call_tool": "call_tool", "end": END})
            graph.add_edge("call_tool", "agent")

            runnable = graph.compile()
            print("ğŸ¤– ëŒ€ì¶œ ì‹¬ì‚¬ AI ì—ì´ì „íŠ¸(SmolLM3)ê°€ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤. ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”? (ì˜ˆ: ì§€ì›ì A001 ì •ë³´ ì•Œë ¤ì¤˜)")

            while True:
                user_input = input("> ")
                if user_input.lower() in ["exit", "quit"]:
                    break

                initial_state = {"messages": [HumanMessage(content=user_input)]}

                # ë¹„ë™ê¸°ë¡œ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì²˜ë¦¬
                async for output in runnable.astream(initial_state):
                    for key, value in output.items():
                        if key == "agent":
                            last_message = value['messages'][-1]
                            if last_message.tool_calls:
                                tool_call = last_message.tool_calls[0]
                                print(f"  ... âš™ï¸ LLMì´ '{tool_call['name']}' ë„êµ¬ í˜¸ì¶œì„ ê²°ì •í–ˆìŠµë‹ˆë‹¤. (ID: {tool_call['id']})")
                            elif last_message.content:
                                print(f"\nğŸ¤– Agent: {last_message.content}")

                        elif key == "call_tool":
                            print(f"  ... âœ… Tool ì‹¤í–‰ ì™„ë£Œ. ê²°ê³¼ë¥¼ LLMì— ì „ë‹¬í•˜ì—¬ ìµœì¢… ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.")


if __name__ == "__main__":
    if sys.platform == 'win32':
        asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
    # í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.
    # ì˜ˆ: pip install torch transformers langchain langchain_huggingface
    asyncio.run(main())