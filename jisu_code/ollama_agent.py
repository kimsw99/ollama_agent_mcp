# ollama_agent.py

import asyncio
import sys
import os
import json
from typing import Annotated, List, Any, TypedDict
from functools import partial

from langchain_core.messages import AnyMessage, HumanMessage, AIMessage, ToolMessage
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.tools import tool
from langgraph.graph import StateGraph, END
from langgraph.checkpoint.memory import MemorySaver

# ìˆ˜ì •: langchain_community -> langchain_ollama
from langchain_ollama.chat_models import ChatOllama

from mcp.client.session import ClientSession
from mcp.client.stdio import StdioServerParameters, stdio_client


# --- 1. Agent ìƒíƒœ ì •ì˜ ---
class AgentState(TypedDict):
    messages: Annotated[list[AnyMessage], lambda x, y: x + y]


# --- 2. LangGraph ì›Œí¬í”Œë¡œìš° í•¨ìˆ˜ ì •ì˜ ---
def should_continue(state: AgentState) -> str:
    last_message = state["messages"][-1]
    if last_message.tool_calls:
        return "call_tool"
    return "end"


def call_model(state: AgentState, agent_runnable):
    response = agent_runnable.invoke(state)
    return {"messages": [response]}


async def call_tool_node(state: AgentState, tools_by_name: dict):
    tool_results = []
    last_message = state["messages"][-1]

    for tool_call in last_message.tool_calls:
        tool_name = tool_call["name"]
        selected_tool = tools_by_name.get(tool_name)

        if not selected_tool:
            result_content = f"ì˜¤ë¥˜: '{tool_name}' ë„êµ¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        else:
            # LLMì´ ì œê³µí•œ ì¸ì(args)ë§Œìœ¼ë¡œ ë„êµ¬ ì‹¤í–‰
            result_content = await selected_tool.ainvoke(tool_call["args"])

        tool_results.append(ToolMessage(content=str(result_content), tool_call_id=tool_call["id"]))

    return {"messages": tool_results}


# --- 3. ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ ---
async def main():
    llm = ChatOllama(model="qwen2:7b-instruct-q4_0", temperature=0)
    print("âœ… Ollama LLM (qwen2:7b-instruct-q4_0)ì´ ì—°ê²°ë˜ì—ˆìŠµë‹ˆë‹¤.")

    server_params = StdioServerParameters(
        command=sys.executable,
        args=["server.py"],
        env=dict(os.environ, PYTHONUNBUFFERED="1")
    )

    async with stdio_client(server_params) as (read, write):
        async with ClientSession(read, write) as mcp_client:
            await mcp_client.initialize()
            print("âœ… MCP ì„¸ì…˜ ì´ˆê¸°í™” ì„±ê³µ!")

            # --- í•µì‹¬ ìˆ˜ì • ---
            # mcp_client ê°ì²´ê°€ ìƒì„±ëœ *ì´í›„ì—* ì´ ê°ì²´ë¥¼ ì‚¬ìš©í•˜ëŠ” ë„êµ¬ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.
            # ì´ë ‡ê²Œ í•˜ë©´ í•¨ìˆ˜ ì‹œê·¸ë‹ˆì²˜ì—ì„œ clientë¥¼ ì œê±°í•˜ì—¬ Pydantic ì˜¤ë¥˜ë¥¼ í”¼í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

            @tool
            async def read_applicant_info(applicant_id: str) -> str:
                """
                ì£¼ì–´ì§„ ì§€ì›ì ID(applicant_id)ì— í•´ë‹¹í•˜ëŠ” ì§€ì›ìì˜ ìƒì„¸ ì •ë³´ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.
                ì‹ ì²­ìì˜ ì†Œë“, ì‹ ìš© ì ìˆ˜, ê¸°ì¡´ ë¶€ì±„ ë“±ì˜ ì •ë³´ë¥¼ í™•ì¸í•˜ê³  ì‹¶ì„ ë•Œ ì‚¬ìš©í•˜ì„¸ìš”.
                """
                try:
                    print(f"  ... âš™ï¸ MCP ë„êµ¬ ì‹¤í–‰: read_applicant_info(applicant_id='{applicant_id}')")
                    # í•¨ìˆ˜ ë°”ê¹¥ì˜ mcp_clientë¥¼ ì‚¬ìš©
                    resource = await mcp_client.read_resource(f"applicant://{applicant_id}")
                    if resource.contents:
                        content = resource.contents[0]
                        if hasattr(content, 'text'):
                            data = json.loads(content.text)
                            return f"ì§€ì›ì {applicant_id} ì •ë³´:\n{json.dumps(data, indent=2, ensure_ascii=False)}"
                    return f"ì§€ì›ì {applicant_id} ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                except Exception as e:
                    return f"ë¦¬ì†ŒìŠ¤ ì½ê¸° ì‹¤íŒ¨: {e}"

            @tool
            async def evaluate_loan_application(applicant_id: str) -> str:
                """
                ì£¼ì–´ì§„ ì§€ì›ì ID(applicant_id)ì— ëŒ€í•´ ëŒ€ì¶œ ì‹ ì²­ì„ ìµœì¢… ì‹¬ì‚¬í•©ë‹ˆë‹¤.
                ì´ ë„êµ¬ëŠ” ì§€ì›ìì˜ ëª¨ë“  ì •ë³´ë¥¼ ì¢…í•©í•˜ì—¬ ëŒ€ì¶œ ìŠ¹ì¸(approve), ë³´ë¥˜(refer), ê±°ì ˆ(decline) ì—¬ë¶€ì™€
                ê·¸ì— ë”°ë¥¸ ì ìˆ˜, ì‚¬ìœ ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤. ëŒ€ì¶œ ê°€ëŠ¥ ì—¬ë¶€ë¥¼ ìµœì¢… ê²°ì •í•  ë•Œ ì‚¬ìš©í•˜ì„¸ìš”.
                """
                try:
                    print(f"  ... âš™ï¸ MCP ë„êµ¬ ì‹¤í–‰: evaluate_loan_application(applicant_id='{applicant_id}')")
                    # í•¨ìˆ˜ ë°”ê¹¥ì˜ mcp_clientë¥¼ ì‚¬ìš©
                    result = await mcp_client.call_tool("evaluate_application", {"applicant_id": applicant_id})
                    if result.content:
                        content = result.content[0]
                        if hasattr(content, 'text'):
                            evaluation = json.loads(content.text)
                            return f"ì‹¬ì‚¬ ê²°ê³¼:\n{json.dumps(evaluation, indent=2, ensure_ascii=False)}"
                    return f"{applicant_id} í‰ê°€ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."
                except Exception as e:
                    return f"{applicant_id} í‰ê°€ ì‹¤íŒ¨: {e}"

            tools = [read_applicant_info, evaluate_loan_application]
            llm_with_tools = llm.bind_tools(tools)

            prompt = ChatPromptTemplate.from_messages([
                ("system",
                 "ë‹¹ì‹ ì€ ìš°ë¦¬ì€í–‰ì˜ ìœ ëŠ¥í•œ ëŒ€ì¶œ ì‹¬ì‚¬ ë³´ì¡° AI ì—ì´ì „íŠ¸ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µí•˜ê¸° ìœ„í•´ ì£¼ì–´ì§„ ë„êµ¬ë¥¼ ì ê·¹ì ìœ¼ë¡œ ì‚¬ìš©í•˜ì„¸ìš”. ìµœì¢… ë‹µë³€ì€ í•­ìƒ í•œêµ­ì–´ë¡œ í•´ì•¼ í•©ë‹ˆë‹¤."),
                MessagesPlaceholder(variable_name="messages"),
            ])

            agent_runnable = prompt | llm_with_tools
            tools_by_name = {t.name: t for t in tools}

            graph = StateGraph(AgentState)
            graph.add_node("agent", partial(call_model, agent_runnable=agent_runnable))
            graph.add_node("call_tool", partial(call_tool_node, tools_by_name=tools_by_name))
            graph.set_entry_point("agent")
            graph.add_conditional_edges("agent", should_continue, {"call_tool": "call_tool", "end": END})
            graph.add_edge("call_tool", "agent")

            runnable = graph.compile(checkpointer=MemorySaver())

            print("\nğŸ¤– ëŒ€ì¶œ ì‹¬ì‚¬ AI ì—ì´ì „íŠ¸(Ollama/Qwen2)ê°€ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤. ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”? (ì˜ˆ: ì§€ì›ì A001 ì •ë³´ ì•Œë ¤ì¤˜)")
            while True:
                user_input = input("> ")
                if user_input.lower() in ["exit", "quit"]:
                    break

                config = {"configurable": {"thread_id": "user_session"}}
                initial_state = {"messages": [HumanMessage(content=user_input)]}

                async for output in runnable.astream(initial_state, config=config):
                    agent_output = output.get('agent')
                    if agent_output:
                        last_message = agent_output.get('messages', [])[-1]
                        if isinstance(last_message, AIMessage) and not last_message.tool_calls:
                            print(f"\nğŸ¤– Agent: {last_message.content}")


if __name__ == "__main__":
    if sys.platform == 'win32':
        asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())

    try:
        import langchain, langchain_community, langchain_core, langgraph, fastmcp, langchain_ollama
    except ImportError:
        print("í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜í•©ë‹ˆë‹¤...")
        os.system(
            f"{sys.executable} -m pip install langchain langchain_community langchain_core langgraph langchain-ollama fastmcp")
        print("ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ ì™„ë£Œ! ìŠ¤í¬ë¦½íŠ¸ë¥¼ ë‹¤ì‹œ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
        sys.exit(0)

    asyncio.run(main())